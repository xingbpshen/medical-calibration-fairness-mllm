azure_openai:
  api_key: ""  # your Azure OpenAI Service API key, leave it empty if you are using local MLLM
  endpoint_url: ""  # your Azure OpenAI Service endpoint, leave it empty if you are using local MLLM
  deployment_name: ""  # your Azure OpenAI Service deployment name, leave it empty if you are using local MLLM
  api_version: ""   # the API version, e.g., "2025-04-01-preview", leave it empty if you are using local MLLM

google_vertex_ai:
  api_key: ""  # your Google Cloud API key, leave it empty if you are using local MLLM

local_mllm:
  model_path: ""  # local model path or HuggingFace repo e.g., "/XXX/Qwen2.5-VL-72B-Instruct" or "Qwen/Qwen2.5-VL-72B-Instruct"
  cache_dir: ""  # local model cache directory, e.g., "/XXX/.cache", leave it empty to use default cache directory
  use_vllm_serve: true  # use vLLM for local MLLM, do not change this
  host: localhost
  port: 8000  # local MLLM port, can be adjusted based on your hardware, e.g., 8000
  cuda_visible_devices: ""  # CUDA devices for local MLLM, can be adjusted based on your hardware, e.g., "0,1"
  tensor_parallel_size: 8  # tensor parallel size, can be adjusted based on your hardware, e.g., 1, 2, 4, etc.
  gpu_memory_utilization: 0.9  # GPU memory utilization, can be adjusted based on your hardware, e.g., 0.9